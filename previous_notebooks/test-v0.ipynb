{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5aba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"hmr4d/\")\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from einops import einsum\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import imageio.v3 as iio\n",
    "import torch\n",
    "\n",
    "from hmr4d.utils.preproc import Tracker, Extractor, VitPoseExtractor, SimpleVO\n",
    "from hmr4d.utils.geo.hmr_cam import get_bbx_xys_from_xyxy, estimate_K, create_camera_sensor\n",
    "from hmr4d.utils.geo.hmr_cam import compute_bbox_info_bedlam, compute_transl_full_cam, normalize_kp2d\n",
    "from hmr4d.utils.geo_transform import compute_cam_angvel, apply_T_on_points, compute_T_ayfz2ay\n",
    "from hmr4d.utils.body_model import BodyModelSMPLH, BodyModelSMPLX\n",
    "from hmr4d.utils.vis.renderer import Renderer, get_global_cameras_static, get_ground_params_from_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c14640",
   "metadata": {},
   "source": [
    "### 1. parse_args_to_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d95ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs/bodycam/Clip 1 1 Clip 1 2 Axon Body 4 Video 2024 12 06 1558 D01A61897 [P1-Va9VIZsI].webm: 960x720x271\n"
     ]
    }
   ],
   "source": [
    "# demo.py -> L41~55\n",
    "video_path = Path(\"inputs/bodycam/Clip 1 1 Clip 1 2 Axon Body 4 Video 2024 12 06 1558 D01A61897 [P1-Va9VIZsI].webm\")\n",
    "output_root = Path(\"outputs/demo\")\n",
    "static_cam = False\n",
    "use_dpvo = False\n",
    "f_mm = None\n",
    "\n",
    "# demo.py -> L60\n",
    "# hmr4d.utils.video_io_utils.get_video_lwh\n",
    "length, height, width, c = iio.improps(video_path, plugin=\"pyav\").shape\n",
    "if length == 0:\n",
    "    video = cv2.VideoCapture(str(video_path))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    length = int(video.get(cv2.CAP_PROP_FRAME_COUNT))-1\n",
    "print(f\"{str(video_path)}: {width}x{height}x{length}\")\n",
    "# demo.py -> L64~79\n",
    "# Cfg\n",
    "# import hydra\n",
    "# from hydra import compose, initialize_config_module\n",
    "# from hmr4d.configs import store_gvhmr\n",
    "\n",
    "# with initialize_config_module(version_base=\"1.3\", config_module=f\"hmr4d.configs\"):\n",
    "#     overrides = [\n",
    "#         f\"video_name={video_path.stem}\",\n",
    "#         f\"static_cam={static_cam}\",\n",
    "#         f\"verbose={True}\",\n",
    "#         f\"use_dpvo={use_dpvo}\",\n",
    "#         f\"f_mm={f_mm}\",\n",
    "#         f\"output_root={output_root}\",\n",
    "#     ]\n",
    "#     cfg = compose(config_name=\"demo\", overrides=overrides)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f51e1",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>CFG: Click to expand</summary>\n",
    "'pipeline': <br/>\n",
    "&emsp;'_target_': 'hmr4d.model.gvhmr.pipeline.gvhmr_pipeline.Pipeline' <br/>\n",
    "&emsp;'args_denoiser3d': '${network}' <br/>\n",
    "&emsp;'args': <br/>\n",
    "&emsp;&emsp;'endecoder_opt': '${endecoder}' <br/>\n",
    "&emsp;&emsp;'normalize_cam_angvel': True <br/>\n",
    "&emsp;&emsp;'weights': None <br/>\n",
    "&emsp;&emsp;'static_conf': None <br/>\n",
    "'ckpt_path': 'inputs/checkpoints/gvhmr/gvhmr_siga24_release.ckpt' <br/> \n",
    "'video_name': 'tennis' <br/>\n",
    "'output_root': 'outputs/demo' <br/>\n",
    "'output_dir': '${output_root}/${video_name}' <br/>\n",
    "'preprocess_dir': '${output_dir}/preprocess' <br/>\n",
    "'video_path': '${output_dir}/0_input_video.mp4' <br/>\n",
    "'static_cam': False <br/>\n",
    "'verbose': True <br/>\n",
    "'use_dpvo': True <br/>\n",
    "'f_mm': 'None' <br/>\n",
    "'paths': {\n",
    "    'bbx': '${preprocess_dir}/bbx.pt' <br/>\n",
    "    'bbx_xyxy_video_overlay': '${preprocess_dir}/bbx_xyxy_video_overlay.mp4' <br/>\n",
    "    'vit_features': '${preprocess_dir}/vit_features.pt' <br/>\n",
    "    'vitpose': '${preprocess_dir}/vitpose.pt' <br/> \n",
    "    'vitpose_video_overlay': '${preprocess_dir}/vitpose_video_overlay.mp4' <br/>\n",
    "    'hmr4d_results': '${output_dir}/hmr4d_results.pt' <br/>\n",
    "    'incam_video': '${output_dir}/1_incam.mp4' <br/>\n",
    "    'global_video': '${output_dir}/2_global.mp4' <br/>\n",
    "    'incam_global_horiz_video': '${output_dir}/${video_name}_3_incam_global_horiz.mp4' <br/>\n",
    "    'slam': '${preprocess_dir}/slam_results.pt'\n",
    "}, \n",
    "'model': {'_target_': 'hmr4d.model.gvhmr.gvhmr_pl_demo.DemoPL', 'pipeline': '${pipeline}'} <br/> \n",
    "'network': {\n",
    "    '_target_': 'hmr4d.network.gvhmr.relative_transformer.NetworkEncoderRoPE', \n",
    "    'output_dim': 151, \n",
    "    'max_len': 120, \n",
    "    'cliffcam_dim': 3, \n",
    "    'cam_angvel_dim': 6, \n",
    "    'imgseq_dim': 1024, \n",
    "    'latent_dim': 512, \n",
    "    'num_layers': 12, \n",
    "    'num_heads': 8, \n",
    "    'mlp_ratio': 4.0, \n",
    "    'pred_cam_dim': 3, \n",
    "    'static_conf_dim': 6, \n",
    "    'dropout': 0.1, \n",
    "    'avgbeta': True} <br/>\n",
    "    'endecoder': { <br/>\n",
    "    '_target_': 'hmr4d.model.gvhmr.utils.endecoder.EnDecoder', <br/>\n",
    "    'stats_name': 'MM_V1_AMASS_LOCAL_BEDLAM_CAM', <br/>\n",
    "    'noise_pose_k': 10} <br/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd95d4fa",
   "metadata": {},
   "source": [
    "### 2. Run Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5059d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YoloV8 Tracking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.050s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YoloV8 Tracking: 271it [00:23, 11.39it/s]\n"
     ]
    }
   ],
   "source": [
    "tracker = Tracker()\n",
    "# bbx_xyxy = tracker.get_one_track(video_path).float()  # (L, 4)\n",
    "track_history = tracker.track(video_path)\n",
    "id_to_frame_ids, id_to_bbx_xyxys, id_sorted = tracker.sort_track_length(track_history, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb43ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.utils.seq_utils import (\n",
    "    get_frame_id_list_from_mask,\n",
    "    linear_interpolate_frame_ids,\n",
    ")\n",
    "from hmr4d.utils.net_utils import moving_average_smooth\n",
    "\n",
    "track_id = 1\n",
    "frame_ids = torch.tensor(id_to_frame_ids[track_id])  # (N,)\n",
    "bbx_xyxys = torch.tensor(id_to_bbx_xyxys[track_id])  # (N, 4)\n",
    "\n",
    "# mask = frame_id_to_mask(frame_ids, length)\n",
    "mask = torch.zeros(length, dtype=torch.bool)\n",
    "mask[frame_ids] = True\n",
    "# bbx_xyxy_one_track = rearrange_by_mask(bbx_xyxys, mask)  # (F, 4), missing filled with 0\n",
    "bbx_xyxy_one_track = torch.zeros((length, 4), dtype=bbx_xyxys.dtype)\n",
    "bbx_xyxy_one_track[mask] = bbx_xyxys\n",
    "missing_frame_id_list = get_frame_id_list_from_mask(~mask)  # list of list\n",
    "bbx_xyxy_one_track = linear_interpolate_frame_ids(bbx_xyxy_one_track, missing_frame_id_list)\n",
    "\n",
    "bbx_xyxy_one_track = moving_average_smooth(bbx_xyxy_one_track, window_size=5, dim=0)\n",
    "bbx_xyxy_one_track = moving_average_smooth(bbx_xyxy_one_track, window_size=5, dim=0)\n",
    "bbx_xyxy = bbx_xyxy_one_track\n",
    "bbx_xys = get_bbx_xys_from_xyxy(bbx_xyxy, base_enlarge=1.2).float()  # (L, 3) apply aspect ratio and enlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864947a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.utils.video_io_utils import read_video_np, save_video\n",
    "from hmr4d.utils.vis.cv2_utils import draw_bbx_xyxy_on_image_batch\n",
    "\n",
    "video = read_video_np(video_path)\n",
    "video_overlay = np.stack(draw_bbx_xyxy_on_image_batch(bbx_xyxy, video), axis=0)\n",
    "save_video(video_overlay, 'tmp.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Assuming 'video_frames' is a numpy array of shape (frames, height, width, channels)\n",
    "# ... code to generate video_frames ...\n",
    "\n",
    "fig = plt.figure()\n",
    "im = plt.imshow(video_overlay[0,:,:,:])\n",
    "plt.close() # Prevents the initial frame from displaying separately\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video_overlay[i,:,:,:])\n",
    "    return [im]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=video_overlay.shape[0], interval=50) # 50ms interval = 20 fps\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b2795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YoloV8 Tracking: 271it [00:11, 22.77it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index is out of bounds for dimension with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# demo.py -> L110~112\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Bbox Tracking (Yolov8x)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tracker \u001b[38;5;241m=\u001b[39m Tracker()\n\u001b[0;32m----> 5\u001b[0m bbx_xyxy \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_one_track\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# (L, 4)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m bbx_xys \u001b[38;5;241m=\u001b[39m get_bbx_xys_from_xyxy(bbx_xyxy, base_enlarge\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# (L, 3) apply aspect ratio and enlarge\u001b[39;00m\n",
      "File \u001b[0;32m/lustre/fs1/home/yo564250/workspace/3D/GVHMR/hmr4d/utils/preproc/tracker.py:86\u001b[0m, in \u001b[0;36mTracker.get_one_track\u001b[0;34m(self, video_path)\u001b[0m\n\u001b[1;32m     83\u001b[0m bbx_xyxys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(id_to_bbx_xyxys[track_id])  \u001b[38;5;66;03m# (N, 4)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# interpolate missing frames\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mframe_id_to_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_video_lwh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m bbx_xyxy_one_track \u001b[38;5;241m=\u001b[39m rearrange_by_mask(bbx_xyxys, mask)  \u001b[38;5;66;03m# (F, 4), missing filled with 0\u001b[39;00m\n\u001b[1;32m     88\u001b[0m missing_frame_id_list \u001b[38;5;241m=\u001b[39m get_frame_id_list_from_mask(\u001b[38;5;241m~\u001b[39mmask)  \u001b[38;5;66;03m# list of list\u001b[39;00m\n",
      "File \u001b[0;32m/lustre/fs1/home/yo564250/workspace/3D/GVHMR/hmr4d/utils/seq_utils.py:121\u001b[0m, in \u001b[0;36mframe_id_to_mask\u001b[0;34m(frame_id, max_len)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mframe_id_to_mask\u001b[39m(frame_id, max_len):\n\u001b[1;32m    120\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(max_len, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[0;32m--> 121\u001b[0m     \u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframe_id\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "\u001b[0;31mIndexError\u001b[0m: index is out of bounds for dimension with size 0"
     ]
    }
   ],
   "source": [
    "# # demo.py -> L110~112\n",
    "# # Bbox Tracking (Yolov8x)\n",
    "\n",
    "# tracker = Tracker()\n",
    "# # bbx_xyxy = tracker.get_one_track(video_path).float()  # (L, 4)\n",
    "# track_history = tracker.track(video_path)\n",
    "# id_to_frame_ids, id_to_bbx_xyxys, id_sorted = tracker.sort_track_length(track_history, video_path)\n",
    "\n",
    "# bbx_xys = get_bbx_xys_from_xyxy(bbx_xyxy, base_enlarge=1.2).float()  # (L, 3) apply aspect ratio and enlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1cce566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViTPose: 100%|██████████| 17/17 [00:11<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# demo.py -> L126~127\n",
    "# Keypoint Extracting (ViTPose)\n",
    "\n",
    "vitpose_extractor = VitPoseExtractor()\n",
    "vitpose = vitpose_extractor.extract(str(video_path), bbx_xys) # (L, 17, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f5394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HMR2 Feature:   0%|          | 0/70 [00:00<?, ?it/s]/home/yo564250/conda-envs/hmr/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "HMR2 Feature: 100%|██████████| 70/70 [00:23<00:00,  2.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# demo.py -> 140~141\n",
    "\n",
    "extractor = Extractor()\n",
    "vit_features = extractor.extract_video_features(str(video_path), bbx_xys) # (L, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "149fe3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SimpleVO] Choosen frames shape: (140, 1080, 1920, 3)\n",
      "TwoViewGeometryOptions:\n",
      "    min_num_inliers = 10\n",
      "    min_E_F_inlier_ratio = 0.8\n",
      "    max_H_inlier_ratio = 0.9\n",
      "    watermark_min_inlier_ratio = 0.7\n",
      "    watermark_border_size = 0.1\n",
      "    detect_watermark = True\n",
      "    multiple_ignore_watermark = True\n",
      "    watermark_detection_max_error = 4.0\n",
      "    filter_stationary_matches = False\n",
      "    stationary_matches_max_error = 4.0\n",
      "    force_H_use = False\n",
      "    compute_relative_pose = True\n",
      "    multiple_models = False\n",
      "    ransac: RANSACOptions:\n",
      "        max_error = 4.0\n",
      "        min_inlier_ratio = 0.25\n",
      "        confidence = 0.999\n",
      "        dyn_num_trials_multiplier = 3.0\n",
      "        min_num_trials = 100\n",
      "        max_num_trials = 10000\n",
      "        random_seed = -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [02:02<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# demo.py -> 151~152\n",
    "\n",
    "simple_vo = SimpleVO(video_path, scale=0.5, step=8, method=\"sift\", f_mm=None)\n",
    "vo_results = simple_vo.compute() # (L, 4, 4)\n",
    "R_w2c = torch.from_numpy(vo_results[:, :3, :3]) # (L, 3, 3)\n",
    "K_fullimg = estimate_K(width, height).repeat(length, 1, 1) # (L, 3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b349e",
   "metadata": {},
   "source": [
    "### 3. Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c8f6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.network.gvhmr.relative_transformer import NetworkEncoderRoPE\n",
    "from hmr4d.model.gvhmr.utils.endecoder import EnDecoder\n",
    "from hmr4d.model.gvhmr.pipeline.gvhmr_pipeline import get_smpl_params_w_Rt_v2\n",
    "from hmr4d.model.gvhmr.utils.postprocess import (\n",
    "    pp_static_joint,\n",
    "    process_ik,\n",
    "    pp_static_joint_cam,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4ccb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m12/12 14:09:17\u001b[0m][\u001b[32mINFO\u001b[0m] [EnDecoder] Use MM_V1_AMASS_LOCAL_BEDLAM_CAM for statistics!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from hmr4d.model.gvhmr.gvhmr_pl_demo import DemoPL\n",
    "# model: DemoPL = hydra.utils.instantiate(cfg.model, _recursive_=False)\n",
    "# model.load_pretrained_model(cfg.ckpt_path)\n",
    "# model = model.eval().cuda()\n",
    "\n",
    "denoiser3d = NetworkEncoderRoPE().eval().cuda()\n",
    "endecoder = EnDecoder(stats_name=\"MM_V1_AMASS_LOCAL_BEDLAM_CAM\").eval().cuda()\n",
    "\n",
    "cam_angvel_mean = torch.tensor([1., 0., 0., 0., 1., 0.], device='cuda')\n",
    "cam_angvel_std = torch.tensor( [1e-3, 0.1, 0.1, 0.1, 1e-3, 0.1], device='cuda')\n",
    "\n",
    "state_dict = torch.load('inputs/checkpoints/gvhmr/gvhmr_siga24_release.ckpt')['state_dict']\n",
    "transf_state_dict = {key.replace(\"pipeline.denoiser3d.\", \"\"): value for key, value in state_dict.items() if \"pipeline.denoiser3d.\" in key}\n",
    "denoiser3d.load_state_dict(transf_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.utilities.memory import recursive_detach\n",
    "\n",
    "# pred = model.predict({\n",
    "#     \"length\": torch.tensor(length),\n",
    "#     \"bbx_xys\": bbx_xys,\n",
    "#     \"kp2d\": vitpose,\n",
    "#     \"K_fullimg\": K_fullimg,\n",
    "#     \"cam_angvel\": compute_cam_angvel(R_w2c),\n",
    "#     \"f_imgseq\": vit_features,\n",
    "# }, static_cam=static_cam)\n",
    "# pred = recursive_detach(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfff5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cliff_cam = compute_bbox_info_bedlam(bbx_xys[None], K_fullimg[None]).to('cuda')  # (B, L, 3)\n",
    "f_cam_angvel = compute_cam_angvel(R_w2c).to('cuda')\n",
    "f_condition = {\n",
    "    \"obs\": normalize_kp2d(vitpose, bbx_xys)[None].to('cuda'),  # (B, L, J, 3)\n",
    "    \"f_cliffcam\": cliff_cam,  # (B, L, 3)\n",
    "    \"f_cam_angvel\": (f_cam_angvel - cam_angvel_mean) / cam_angvel_std,  # (B, L, C=6)\n",
    "    \"f_imgseq\": vit_features[None].to('cuda'),  # (B, L, C=1024)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03db56a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = denoiser3d(length=torch.tensor([length], device='cuda'), **f_condition)  # pred_x, pred_cam, static_conf_logits\n",
    "    decode_dict = endecoder.decode(model_output[\"pred_x\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35df0ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "# Post-processing\n",
    "outputs[\"pred_smpl_params_incam\"] = {\n",
    "    \"body_pose\": decode_dict[\"body_pose\"],  # (B, L, 63)\n",
    "    \"betas\": decode_dict[\"betas\"],  # (B, L, 10)\n",
    "    \"global_orient\": decode_dict[\"global_orient\"],  # (B, L, 3)\n",
    "    \"transl\": compute_transl_full_cam(\n",
    "        model_output[\"pred_cam\"], bbx_xys.to('cuda'), K_fullimg.to('cuda')),\n",
    "}\n",
    "\n",
    "pred_smpl_params_global = get_smpl_params_w_Rt_v2(  # This function has for-loop\n",
    "    global_orient_gv=decode_dict[\"global_orient_gv\"],\n",
    "    local_transl_vel=decode_dict[\"local_transl_vel\"],\n",
    "    global_orient_c=decode_dict[\"global_orient\"],\n",
    "    cam_angvel=f_cam_angvel[None],\n",
    ")\n",
    "outputs[\"pred_smpl_params_global\"] = {\n",
    "    \"body_pose\": decode_dict[\"body_pose\"],\n",
    "    \"betas\": decode_dict[\"betas\"],\n",
    "    **pred_smpl_params_global,\n",
    "}\n",
    "outputs[\"static_conf_logits\"] = model_output[\"static_conf_logits\"]\n",
    "\n",
    "if static_cam:  # extra post-processing to utilize static camera prior\n",
    "    outputs[\"pred_smpl_params_global\"][\"transl\"] = pp_static_joint_cam(outputs, endecoder)\n",
    "else:\n",
    "    outputs[\"pred_smpl_params_global\"][\"transl\"] = pp_static_joint(outputs, endecoder)\n",
    "\n",
    "body_pose = process_ik(outputs, endecoder)\n",
    "decode_dict[\"body_pose\"] = body_pose\n",
    "outputs[\"pred_smpl_params_global\"][\"body_pose\"] = body_pose\n",
    "outputs[\"pred_smpl_params_incam\"][\"body_pose\"] = body_pose\n",
    "\n",
    "pred = {\n",
    "    \"smpl_params_global\": {k: v[0] for k, v in outputs[\"pred_smpl_params_global\"].items()},\n",
    "    \"smpl_params_incam\": {k: v[0] for k, v in outputs[\"pred_smpl_params_incam\"].items()},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27ac3d",
   "metadata": {},
   "source": [
    "### 4. Render InCamera View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a604167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl = BodyModelSMPLH(\n",
    "    model_path=\"inputs/checkpoints/body_models\", model_type=\"smpl\",\n",
    "    gender=\"neutral\", num_betas=10, create_body_pose=False, \n",
    "    create_betas=False, create_global_orient=False, create_transl=False,\n",
    ").cuda()\n",
    "smplx = BodyModelSMPLX(\n",
    "    model_path=\"inputs/checkpoints/body_models\", model_type=\"smplx\",\n",
    "    gender=\"neutral\", num_pca_comps=12, flat_hand_mean=False,\n",
    ").cuda()\n",
    "smplx2smpl = torch.load(\"hmr4d/utils/body_model/smplx2smpl_sparse.pt\").cuda()\n",
    "faces_smpl = smpl.faces\n",
    "J_regressor = torch.load(\"hmr4d/utils/body_model/smpl_neutral_J_regressor.pt\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b62815d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smpl\n",
    "smplx_out = smplx(**pred[\"smpl_params_incam\"])\n",
    "pred_c_verts = torch.stack([torch.matmul(smplx2smpl, v_) for v_ in smplx_out.vertices])\n",
    "\n",
    "renderer_c = Renderer(width, height, device=\"cuda\", faces=faces_smpl, K=K_fullimg[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db6810b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "smplx_out = smplx(**pred[\"smpl_params_global\"])\n",
    "pred_ay_verts = torch.stack([torch.matmul(smplx2smpl, v_) for v_ in smplx_out.vertices])\n",
    "\n",
    "pred_ay_verts = pred_ay_verts.clone()  # (L, V, 3)\n",
    "offset = einsum(J_regressor, pred_ay_verts[0], \"j v, v i -> j i\")[0]  # (3)\n",
    "offset[1] = pred_ay_verts[:, :, [1]].min()\n",
    "pred_gb_verts = pred_ay_verts - offset\n",
    "# face direction\n",
    "T_ay2ayfz = compute_T_ayfz2ay(einsum(J_regressor, pred_gb_verts[[0]], \"j v, l v i -> l j i\"), inverse=True)\n",
    "pred_gb_verts = apply_T_on_points(pred_gb_verts, T_ay2ayfz)\n",
    "\n",
    "pred_gb_joints = einsum(J_regressor, pred_gb_verts, \"j v, l v i -> l j i\")  # (L, J, 3)\n",
    "global_R, global_T, global_lights = get_global_cameras_static(\n",
    "    pred_gb_joints.cpu(), beta=2.0, cam_height_degree=20, target_center_height=1.0,\n",
    ")\n",
    "\n",
    "_, _, K = create_camera_sensor(width, height, 24)\n",
    "renderer_g = Renderer(width, height, device=\"cuda\", faces=faces_smpl, K=K)\n",
    "\n",
    "# -- render mesh -- #\n",
    "scale, cx, cz = get_ground_params_from_points(pred_gb_joints[:, 0], pred_gb_verts)\n",
    "renderer_g.set_ground(scale * 1.5, cx, cz)\n",
    "color = torch.ones(3).float().cuda() * 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "813dfbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 49\n",
    "\n",
    "img_raw = iio.imread(video_path, index=i)\n",
    "img_cam = renderer_c.render_mesh(pred_c_verts[i].cuda(), img_raw, [0.8, 0.8, 0.8])\n",
    "cameras = renderer_g.create_camera(global_R[i], global_T[i])\n",
    "img_gb = renderer_g.render_with_ground(pred_gb_verts[[i]], color[None], cameras, global_lights)\n",
    "Image.fromarray(np.concatenate([img_raw, img_cam, img_gb], axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
