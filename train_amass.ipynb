{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d8c920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import imageio.v3 as iio\n",
    "import torch\n",
    "\n",
    "from hmr4d.dataset.pure_motion.amass import AmassDataset #52,788 samples\n",
    "from hmr4d.dataset.pure_motion.utils import augment_betas, interpolate_smpl_params, rotate_around_axis\n",
    "from hmr4d.dataset.pure_motion.cam_traj_utils import CameraAugmentorV11\n",
    "from hmr4d.utils.body_model import BodyModelSMPLH, BodyModelSMPLX\n",
    "from hmr4d.utils.body_model.smplx_lite import SmplxLiteSmplN24\n",
    "from hmr4d.utils.geo.hmr_global import get_c_rootparam, get_R_c2gv, get_tgtcoord_rootparam, get_T_w2c_from_wcparams\n",
    "from hmr4d.utils.geo.hmr_cam import create_camera_sensor\n",
    "from hmr4d.utils.geo_transform import compute_cam_angvel, apply_T_on_points, move_to_start_point_face_z\n",
    "from hmr4d.utils.net_utils import get_valid_mask\n",
    "from hmr4d.utils.wis3d_utils import convert_motion_as_line_mesh\n",
    "from hmr4d.utils.video_io_utils import save_video, get_writer\n",
    "from hmr4d.utils.vis.renderer import Renderer, get_global_cameras_static, get_ground_params_from_points\n",
    "from hmr4d.utils.vis.renderer_utils import simple_render_mesh\n",
    "\n",
    "# [12/23 15:45:21][INFO] [AMASS] 18086 sequences. Elapsed: 2.35s\n",
    "# [12/23 15:45:21][INFO] [AMASS] has 64.7 hours motion -> Resampled to 52788 samples.\n",
    "# [12/23 15:40:11][INFO] [BEDLAM] 37537 sequences. \n",
    "# [12/23 15:40:24][INFO] [H36M] 600 sequences. Elapsed: 0.61s\n",
    "# [12/23 15:40:25][INFO] [H36M] has 8.7 hours motion -> Resampled to 6196 samples. \n",
    "# [12/23 15:46:37][INFO] [3DPW] has 7.5 minutes motion -> Resampled to 88 samples. \n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7273f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl = BodyModelSMPLH(\n",
    "    model_path=\"inputs/checkpoints/body_models\", model_type=\"smpl\",\n",
    "    gender=\"neutral\", num_betas=10, create_body_pose=False, \n",
    "    create_betas=False, create_global_orient=False, create_transl=False,\n",
    ").to(device)\n",
    "smplx = BodyModelSMPLX(\n",
    "    model_path=\"inputs/checkpoints/body_models\", model_type=\"smplx\",\n",
    "    gender=\"neutral\", num_pca_comps=12, flat_hand_mean=False,\n",
    ").to(device)\n",
    "smplx2smpl = torch.load(\"hmr4d/utils/body_model/smplx2smpl_sparse.pt\").to(device)\n",
    "faces_smpl = torch.from_numpy((smpl.faces).astype(\"int\")).unsqueeze(0).to(device)\n",
    "faces_smplx = torch.from_numpy((smplx.faces).astype(\"int\")).unsqueeze(0).to(device)\n",
    "J_regressor = torch.load(\"hmr4d/utils/body_model/smpl_neutral_J_regressor.pt\").to(device)\n",
    "\n",
    "smplx_lite = SmplxLiteSmplN24()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87d50b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m01/01 11:13:54\u001b[0m][\u001b[32mINFO\u001b[0m] [AMASS] Loading from inputs/AMASS/hmr4d_support/smplxpose_v2.pth ...\u001b[0m\n",
      "[\u001b[36m01/01 11:13:56\u001b[0m][\u001b[32mINFO\u001b[0m] [AMASS] 18086 sequences. Elapsed: 2.88s\u001b[0m\n",
      "[\u001b[36m01/01 11:13:56\u001b[0m][\u001b[32mINFO\u001b[0m] [AMASS] has 64.7 hours motion -> Resampled to 52788 samples.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_name': 'amass', 'idx': 5, 'vid': 'inputs/smplx_amass/smplxn_raw/Transitions/Transitions/mazen_c3d/crawl_push_stageii.npz', 'start_end': (129, 255)}\n"
     ]
    }
   ],
   "source": [
    "# Sample Batch from Dataset\n",
    "dataset = AmassDataset()\n",
    "\n",
    "np.random.seed(4)\n",
    "batch = dataset[5]\n",
    "print(batch['meta'])\n",
    "\n",
    "length = batch['length']\n",
    "K = batch['K_fullimg'][0].to(device)\n",
    "width, height = int(K[0,2])*2, int(K[1,2])*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5cbdb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:03<00:00, 31.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Render Test -- Camera, SMPL-X\n",
    "smpl_params_c = {k:v.to(device) for k,v in batch['smpl_params_c'].items()}\n",
    "verts = smplx(**smpl_params_c).vertices\n",
    "\n",
    "renderer_c = Renderer(width, height, device=\"cuda\", faces=smplx.faces, K=K)\n",
    "\n",
    "writer = get_writer('tmp.mp4', fps=30, crf=23)\n",
    "for i in tqdm(range(length)):\n",
    "    img = renderer_c.render_mesh(verts[i], None, [0.8, 0.8, 0.8])\n",
    "    writer.write_frame(img)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9623ca97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering Camera: 100%|██████████| 120/120 [00:02<00:00, 54.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Render Test -- World (Random Camera), Skeleton\n",
    "smpl_params_w = {k:v.to(device) for k,v in batch['smpl_params_w'].items()}\n",
    "w_j3d = smplx(**smpl_params_w).joints.cpu()\n",
    "\n",
    "width, height, K_fullimg = create_camera_sensor(1000, 1000, 24) \n",
    "wham_cam_augmentor = CameraAugmentorV11()\n",
    "T_w2c = wham_cam_augmentor(w_j3d, length) \n",
    "c_j3d = apply_T_on_points(w_j3d[:,:22], T_w2c)\n",
    "# c_j3d = w_j3d\n",
    "verts, faces, vertex_colors = convert_motion_as_line_mesh(c_j3d)\n",
    "vertex_colors = vertex_colors[None] / 255.0\n",
    "renderer = Renderer(width, height, device=\"cuda\", faces=faces, K=K_fullimg)\n",
    "\n",
    "writer = get_writer(f'tmp.mp4', fps=30, crf=23)\n",
    "for i in tqdm(range(120), desc=f\"Rendering Camera\"):\n",
    "    img_overlay_pred = renderer.render_mesh(verts[i].cuda(), None, vertex_colors, VI=1)\n",
    "    writer.write_frame(img_overlay_pred)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654576c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total motion files: 17,896\n",
      "64.7 hours motion -> Resampled to 52,788 samples.\n"
     ]
    }
   ],
   "source": [
    "### AMASS Train Dataset --Load Dataset-- ###\n",
    "\n",
    "motion_frames_len = 120\n",
    "l_factor = 1.5\n",
    "\n",
    "motion_files = torch.load(\"inputs/AMASS/hmr4d_support/smplxpose_v2.pth\")\n",
    "seqs = {k: v for k,v in motion_files.items() if 'moyo_smplxn' not in k and v['pose'].shape[0] >= 25}\n",
    "print(f\"Total motion files: {len(seqs):,}\")\n",
    "\n",
    "hours = 0\n",
    "idx2meta = []\n",
    "for vid, seq in seqs.items():\n",
    "    seq_length = seq[\"pose\"].shape[0]\n",
    "    num_samples = max(seq_length // motion_frames_len, 1)\n",
    "    hours += seq_length\n",
    "    idx2meta.extend([vid] * num_samples)\n",
    "print(f\"{hours / (30*3600):.1f} hours motion -> Resampled to {len(idx2meta):,} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf6f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smplxn_raw/Transitions/Transitions/mazen_c3d/airkick_longjump_stageii.npz : 306 -> 92 ~ 223 (len=131)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'body_pose': tensor[120, 63] n=7560 (30Kb) x∈[-1.788, 1.745] μ=0.031 σ=0.302,\n",
       " 'betas': tensor[120, 10] n=1200 (4.7Kb) x∈[-4.130, 2.712] μ=-0.422 σ=1.990,\n",
       " 'global_orient': tensor[120, 3] n=360 (1.4Kb) x∈[-0.265, 0.419] μ=-0.003 σ=0.138,\n",
       " 'transl': tensor[120, 3] n=360 (1.4Kb) x∈[-0.429, 2.674] μ=0.662 σ=0.885}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### AMASS Train Dataset --Load Data-- ###\n",
    "idx = 10\n",
    "np.random.seed(42)\n",
    "\n",
    "mid = idx2meta[idx]\n",
    "raw_data = seqs[mid]\n",
    "raw_len = raw_data[\"pose\"].shape[0]\n",
    "\n",
    "raw_subset_len = np.random.randint(\n",
    "    int(motion_frames_len / l_factor), int(motion_frames_len * l_factor)\n",
    ")\n",
    "start = np.random.randint(0, raw_len - raw_subset_len + 1) if raw_subset_len <= raw_len else 0\n",
    "end = start + raw_subset_len if raw_subset_len <= raw_len else raw_len\n",
    "print(f\"{'/'.join(Path(mid).parts[2:])} : {raw_len} -> {start} ~ {end} (len={end-start})\")\n",
    "\n",
    "data = {\n",
    "    \"body_pose\": raw_data[\"pose\"][start:end, 3:],  # (F, 63)\n",
    "    \"betas\": raw_data[\"beta\"].repeat(end-start, 1),  # (10)\n",
    "    \"global_orient\": raw_data[\"pose\"][start:end, :3],  # (F, 3)\n",
    "    \"transl\": raw_data[\"trans\"][start:end, :3],  # (F, 3)\n",
    "    \"data_name\" : \"amass\"\n",
    "}\n",
    "data = interpolate_smpl_params(data, motion_frames_len)\n",
    "data[\"global_orient\"], data[\"transl\"], _ = get_tgtcoord_rootparam(\n",
    "    data[\"global_orient\"], data[\"transl\"], tsf=\"az->ay\",\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f85faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### AMASS Train Dataset --Process Data-- ###\n",
    "\n",
    "betas = augment_betas(data[\"betas\"], std=0.1)\n",
    "global_orient_w, transl_w = rotate_around_axis(data[\"global_orient\"], data[\"transl\"], axis=\"y\")\n",
    "smpl_params_w = {\n",
    "    'body_pose' : data[\"body_pose\"], \n",
    "    'betas': betas, \n",
    "    'global_orient': global_orient_w, \n",
    "    'transl': transl_w\n",
    "}\n",
    "\n",
    "## Camera Trajectory Augmentation\n",
    "w_j3d = smplx_lite(\n",
    "    smpl_params_w[\"body_pose\"][::10], betas[::10], global_orient_w[::10], None,\n",
    ")\n",
    "w_j3d = w_j3d.repeat_interleave(10, dim=0) + transl_w[:, None]  # (F, 24, 3)\n",
    "width, height, K_fullimg = create_camera_sensor(1000, 1000, 24) \n",
    "wham_cam_augmentor = CameraAugmentorV11()\n",
    "T_w2c = wham_cam_augmentor(w_j3d, motion_frames_len) \n",
    "\n",
    "offset = smplx.get_skeleton(betas[0].to(device))[0]  # (3)\n",
    "global_orient_c, transl_c = get_c_rootparam(\n",
    "    global_orient_w, transl_w, \n",
    "    T_w2c, offset.cpu(),\n",
    ")\n",
    "smpl_params_c = {\n",
    "    \"body_pose\": smpl_params_w[\"body_pose\"].clone(),  # (F, 63)\n",
    "    \"betas\": smpl_params_w[\"betas\"].clone(),  # (F, 10)\n",
    "    \"global_orient\": global_orient_c,  # (F, 3)\n",
    "    \"transl\": transl_c,  # (F, 3)\n",
    "}\n",
    "        \n",
    "# World Params\n",
    "gravity_vec = torch.tensor([0, -1, 0], dtype=torch.float32)  # (3), BEDLAM is ay\n",
    "R_c2gv = get_R_c2gv(T_w2c[:, :3, :3], gravity_vec)  # (F, 3, 3)\n",
    "\n",
    "K_fullimg = K_fullimg.repeat(motion_frames_len, 1, 1)  # (F, 3, 3)\n",
    "cam_angvel = compute_cam_angvel(T_w2c[:, :3, :3])  # (F, 6)\n",
    "\n",
    "batch = {\n",
    "    \"meta\": {\"data_name\": \"amass\", \"idx\": idx, \"T_w2c\": T_w2c},\n",
    "    \"length\": data[\"body_pose\"].shape[0],\n",
    "    \"smpl_params_c\": smpl_params_c,\n",
    "    \"smpl_params_w\": smpl_params_w,\n",
    "    \"R_c2gv\": R_c2gv,  # (F, 3, 3)\n",
    "    \"gravity_vec\": gravity_vec,  # (3)\n",
    "    \"bbx_xys\": torch.zeros((data[\"body_pose\"].shape[0], 3)),  # (F, 3)  # NOTE: a placeholder\n",
    "    \"K_fullimg\": K_fullimg,  # (F, 3, 3)\n",
    "    \"f_imgseq\": torch.zeros((data[\"body_pose\"].shape[0], 1024)),  # (F, D)  # NOTE: a placeholder\n",
    "    \"kp2d\": torch.zeros(data[\"body_pose\"].shape[0], 17, 3),  # (F, 17, 3)\n",
    "    \"cam_angvel\": cam_angvel,  # (F, 6)\n",
    "    \"mask\": {\n",
    "        \"valid\": get_valid_mask(data[\"body_pose\"].shape[0], data[\"body_pose\"].shape[0]),\n",
    "        \"vitpose\": False,\n",
    "        \"bbx_xys\": False,\n",
    "        \"f_imgseq\": False,\n",
    "        \"spv_incam_only\": False,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87df7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering Camera: 100%|██████████| 120/120 [00:01<00:00, 62.84it/s]\n"
     ]
    }
   ],
   "source": [
    "w_j3d = smplx(**{k:v.to(device) for k,v in smpl_params_w.items()}).joints.cpu()\n",
    "\n",
    "width, height, K_fullimg = create_camera_sensor(1000, 1000, 24) \n",
    "wham_cam_augmentor = CameraAugmentorV11()\n",
    "T_w2c = wham_cam_augmentor(w_j3d, motion_frames_len) \n",
    "c_j3d = apply_T_on_points(w_j3d[:,:22], T_w2c)\n",
    "verts, faces, vertex_colors = convert_motion_as_line_mesh(c_j3d)\n",
    "\n",
    "vertex_colors = vertex_colors[None] / 255.0\n",
    "bg = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "renderer = Renderer(width, height, device=\"cuda\", faces=faces, K=K_fullimg)\n",
    "writer = get_writer(f'tmp.mp4', fps=30, crf=23)\n",
    "for i in tqdm(range(motion_frames_len), desc=f\"Rendering Camera\"):\n",
    "    img_overlay_pred = renderer.render_mesh(verts[i].cuda(), bg, vertex_colors, VI=1)\n",
    "    writer.write_frame(img_overlay_pred)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adabe782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering Global: 100%|██████████| 120/120 [00:04<00:00, 27.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# smplx_out = smplx(**{\n",
    "#     \"body_pose\": data[\"body_pose\"].to(device),  # (F, 63)\n",
    "#     \"betas\": betas.to(device),  # (F, 10)\n",
    "#     \"global_orient\": global_orient_w.to(device),  # (F, 3)\n",
    "#     \"transl\": transl_w.to(device),  # (F, 3)\n",
    "# })\n",
    "smplx_out = smplx(**{k: v.to(device) for k,v in batch['smpl_params_w'].items()})\n",
    "pred_ay_verts = torch.stack([torch.matmul(smplx2smpl, v_) for v_ in smplx_out.vertices])\n",
    "pred_gb_verts, pred_gb_joints = move_to_start_point_face_z(pred_ay_verts, J_regressor)\n",
    "\n",
    "global_R, global_T, global_lights = get_global_cameras_static(\n",
    "    pred_gb_joints.cpu(), beta=2.0, cam_height_degree=20, target_center_height=1.0,\n",
    ")\n",
    "_, _, K = create_camera_sensor(width, height, 24)\n",
    "renderer_g = Renderer(width, height, device=\"cuda\", faces=faces_smpl[0], K=K)\n",
    "\n",
    "# -- render mesh -- #\n",
    "scale, cx, cz = get_ground_params_from_points(pred_gb_joints[:, 0], pred_gb_verts)\n",
    "renderer_g.set_ground(scale * 1.5, cx, cz)\n",
    "color = torch.ones(3).float().cuda() * 0.8\n",
    "\n",
    "writer = get_writer(f'tmp2.mp4', fps=30, crf=23)\n",
    "for i in tqdm(range(batch['length']), desc=f\"Rendering Global\"):\n",
    "    # img_overlay_pred = renderer.render_mesh(verts[i].cuda(), bg, vertex_colors, VI=1)\n",
    "    cameras = renderer_g.create_camera(global_R[i], global_T[i])\n",
    "    img_gb = renderer_g.render_with_ground(pred_gb_verts[[i]], color[None], cameras, global_lights)\n",
    "    writer.write_frame(img_gb)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7dbde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering:   0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering: 100%|██████████| 120/120 [00:02<00:00, 58.34it/s]\n"
     ]
    }
   ],
   "source": [
    "smplx_out = smplx(**{k: v.to(device) for k,v in smpl_params_c.items()})\n",
    "\n",
    "# ----- Render Overlay ----- #\n",
    "render_dict = {\n",
    "    \"faces\": smplx.faces,\n",
    "    \"verts\": smplx_out.vertices,\n",
    "    'whf' : (1280, 720, 995.5555)\n",
    "}\n",
    "img_overlay = simple_render_mesh(render_dict)\n",
    "save_video(img_overlay, \"tmp.mp4\", crf=23)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
