{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8c920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import imageio.v3 as iio\n",
    "import torch\n",
    "\n",
    "from hmr4d.dataset.pure_motion.utils import augment_betas, interpolate_smpl_params, rotate_around_axis\n",
    "from hmr4d.dataset.pure_motion.cam_traj_utils import CameraAugmentorV11\n",
    "from hmr4d.utils.body_model import BodyModelSMPLH, BodyModelSMPLX\n",
    "from hmr4d.utils.body_model.smplx_lite import SmplxLiteSmplN24\n",
    "from hmr4d.utils.geo.hmr_global import get_c_rootparam, get_R_c2gv, get_tgtcoord_rootparam, get_T_w2c_from_wcparams\n",
    "from hmr4d.utils.geo.hmr_cam import create_camera_sensor\n",
    "from hmr4d.utils.geo_transform import compute_cam_angvel, apply_T_on_points, move_to_start_point_face_z\n",
    "from hmr4d.utils.net_utils import get_valid_mask\n",
    "from hmr4d.utils.wis3d_utils import convert_motion_as_line_mesh\n",
    "from hmr4d.utils.video_io_utils import read_video_np, save_video, get_writer\n",
    "from hmr4d.utils.vis.renderer import Renderer, get_global_cameras_static, get_ground_params_from_points\n",
    "from hmr4d.utils.vis.renderer_utils import simple_render_mesh\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7273f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl = BodyModelSMPLH(\n",
    "    model_path=\"inputs/checkpoints/body_models\", model_type=\"smpl\",\n",
    "    gender=\"neutral\", num_betas=10, create_body_pose=False, \n",
    "    create_betas=False, create_global_orient=False, create_transl=False,\n",
    ").to(device)\n",
    "smplx = BodyModelSMPLX(\n",
    "    model_path=\"inputs/checkpoints/body_models\", model_type=\"smplx\",\n",
    "    gender=\"neutral\", num_pca_comps=12, flat_hand_mean=False,\n",
    ").to(device)\n",
    "smplx2smpl = torch.load(\"hmr4d/utils/body_model/smplx2smpl_sparse.pt\").to(device)\n",
    "faces_smpl = smpl.faces\n",
    "J_regressor = torch.load(\"hmr4d/utils/body_model/smpl_neutral_J_regressor.pt\").to(device)\n",
    "\n",
    "smplx_lite = SmplxLiteSmplN24()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62694941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m12/23 22:58:30\u001b[0m][\u001b[32mINFO\u001b[0m] [H36M] Loading from inputs/H36M/hmr4d_support/smplxpose_v1.pt ...\u001b[0m\n",
      "[\u001b[36m12/23 22:58:30\u001b[0m][\u001b[32mINFO\u001b[0m] [H36M] 600 sequences. Elapsed: 0.67s\u001b[0m\n",
      "[\u001b[36m12/23 22:58:30\u001b[0m][\u001b[32mINFO\u001b[0m] [H36M] Fully Loading to RAM ViT-Feat: inputs/H36M/hmr4d_support/vitfeat_h36m.pt\u001b[0m\n",
      "[\u001b[36m12/23 22:58:32\u001b[0m][\u001b[32mINFO\u001b[0m] [H36M] Finished. Elapsed: 1.65s\u001b[0m\n",
      "[\u001b[36m12/23 22:58:32\u001b[0m][\u001b[32mINFO\u001b[0m] [H36M] has 8.7 hours motion -> Resampled to 6196 samples.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# [12/23 15:45:21][INFO] [AMASS] 18086 sequences. Elapsed: 2.35s\n",
    "# [12/23 15:45:21][INFO] [AMASS] has 64.7 hours motion -> Resampled to 52788 samples.\n",
    "# [12/23 15:40:11][INFO] [BEDLAM] 37537 sequences. \n",
    "# [12/23 15:40:24][INFO] [H36M] 600 sequences. Elapsed: 0.61s\n",
    "# [12/23 15:40:25][INFO] [H36M] has 8.7 hours motion -> Resampled to 6196 samples.\n",
    "# [12/23 15:46:37][INFO] [3DPW] has 7.5 minutes motion -> Resampled to 88 samples.\n",
    "\n",
    "from hmr4d.dataset.pure_motion.amass import AmassDataset #52,788 samples\n",
    "from hmr4d.dataset.bedlam.bedlam import BedlamDatasetV2 #37,537 samples\n",
    "from hmr4d.dataset.h36m.h36m import H36mSmplDataset #6,196 samples\n",
    "from hmr4d.dataset.threedpw.threedpw_motion_train import ThreedpwSmplDataset #88 samples\n",
    "\n",
    "dataset = H36mSmplDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5cbdb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:03<00:00, 38.56it/s]\n"
     ]
    }
   ],
   "source": [
    "batch = dataset[500]\n",
    "length = batch['length']\n",
    "\n",
    "K = batch['K_fullimg'][0].to(device)\n",
    "width, height = int(K[0,2])*2, int(K[1,2])*2\n",
    "smpl_params_c = {k:v.to(device) for k,v in batch['smpl_params_c'].items()}\n",
    "verts = smplx(**smpl_params_c).vertices\n",
    "\n",
    "renderer_c = Renderer(width, height, device=\"cuda\", faces=smplx.faces, K=K)\n",
    "\n",
    "writer = get_writer('tmp.mp4', fps=30, crf=23)\n",
    "for i in tqdm(range(length)):\n",
    "    img = renderer_c.render_mesh(verts[i], None, [0.8, 0.8, 0.8])\n",
    "    writer.write_frame(img)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_j3d = smplx(**{k:v.to(device) for k,v in smpl_params_w.items()}).joints.cpu()\n",
    "\n",
    "width, height, K_fullimg = create_camera_sensor(1000, 1000, 24) \n",
    "wham_cam_augmentor = CameraAugmentorV11()\n",
    "T_w2c = wham_cam_augmentor(w_j3d, 120) \n",
    "\n",
    "c_j3d = apply_T_on_points(w_j3d[:,:22], T_w2c)\n",
    "verts, faces, vertex_colors = convert_motion_as_line_mesh(c_j3d)\n",
    "vertex_colors = vertex_colors[None] / 255.0\n",
    "bg = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "renderer = Renderer(width, height, device=\"cuda\", faces=faces, K=K_fullimg)\n",
    "writer = get_writer(f'tmp.mp4', fps=30, crf=23)\n",
    "for i in tqdm(range(120), desc=f\"Rendering Camera\"):\n",
    "    img_overlay_pred = renderer.render_mesh(verts[i].cuda(), bg, vertex_colors, VI=1)\n",
    "    writer.write_frame(img_overlay_pred)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "654576c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total motion files: 17,896\n",
      "64.7 hours motion -> Resampled to 52,788 samples.\n"
     ]
    }
   ],
   "source": [
    "### AMASS Train Dataset --Load Dataset-- ###\n",
    "\n",
    "motion_frames_len = 120\n",
    "l_factor = 1.5\n",
    "\n",
    "motion_files = torch.load(\"inputs/AMASS/hmr4d_support/smplxpose_v2.pth\")\n",
    "seqs = {k: v for k,v in motion_files.items() if 'moyo_smplxn' not in k and v['pose'].shape[0] >= 25}\n",
    "print(f\"Total motion files: {len(seqs):,}\")\n",
    "\n",
    "hours = 0\n",
    "idx2meta = []\n",
    "for vid, seq in seqs.items():\n",
    "    seq_length = seq[\"pose\"].shape[0]\n",
    "    num_samples = max(seq_length // motion_frames_len, 1)\n",
    "    hours += seq_length\n",
    "    idx2meta.extend([vid] * num_samples)\n",
    "print(f\"{hours / (30*3600):.1f} hours motion -> Resampled to {len(idx2meta):,} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37bf6f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smplxn_raw/Transitions/Transitions/mazen_c3d/airkick_longjump_stageii.npz : 306 -> 92 ~ 223 (len=131)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'body_pose': tensor[120, 63] n=7560 (30Kb) x∈[-1.788, 1.745] μ=0.031 σ=0.302,\n",
       " 'betas': tensor[120, 10] n=1200 (4.7Kb) x∈[-4.130, 2.712] μ=-0.422 σ=1.990,\n",
       " 'global_orient': tensor[120, 3] n=360 (1.4Kb) x∈[-0.265, 0.419] μ=-0.003 σ=0.138,\n",
       " 'transl': tensor[120, 3] n=360 (1.4Kb) x∈[-0.429, 2.674] μ=0.662 σ=0.885}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### AMASS Train Dataset --Load Data-- ###\n",
    "idx = 10\n",
    "np.random.seed(42)\n",
    "\n",
    "mid = idx2meta[idx]\n",
    "raw_data = seqs[mid]\n",
    "raw_len = raw_data[\"pose\"].shape[0]\n",
    "\n",
    "raw_subset_len = np.random.randint(\n",
    "    int(motion_frames_len / l_factor), int(motion_frames_len * l_factor)\n",
    ")\n",
    "start = np.random.randint(0, raw_len - raw_subset_len + 1) if raw_subset_len <= raw_len else 0\n",
    "end = start + raw_subset_len if raw_subset_len <= raw_len else raw_len\n",
    "print(f\"{'/'.join(Path(mid).parts[2:])} : {raw_len} -> {start} ~ {end} (len={end-start})\")\n",
    "\n",
    "data = {\n",
    "    \"body_pose\": raw_data[\"pose\"][start:end, 3:],  # (F, 63)\n",
    "    \"betas\": raw_data[\"beta\"].repeat(end-start, 1),  # (10)\n",
    "    \"global_orient\": raw_data[\"pose\"][start:end, :3],  # (F, 3)\n",
    "    \"transl\": raw_data[\"trans\"][start:end, :3],  # (F, 3)\n",
    "    \"data_name\" : \"amass\"\n",
    "}\n",
    "data = interpolate_smpl_params(data, motion_frames_len)\n",
    "data[\"global_orient\"], data[\"transl\"], _ = get_tgtcoord_rootparam(\n",
    "    data[\"global_orient\"], data[\"transl\"], tsf=\"az->ay\",\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f85faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = augment_betas(data[\"betas\"], std=0.1)\n",
    "global_orient_w, transl_w = rotate_around_axis(data[\"global_orient\"], data[\"transl\"], axis=\"y\")\n",
    "smpl_params_w = {'body_pose' : data[\"body_pose\"], 'betas': betas, 'global_orient': global_orient_w, 'transl': transl_w}\n",
    "\n",
    "## Camera Trajectory Augmentation\n",
    "w_j3d = smplx_lite(\n",
    "    data[\"body_pose\"][::10], betas[::10], global_orient_w[::10], None,\n",
    ")\n",
    "w_j3d = w_j3d.repeat_interleave(10, dim=0) + transl_w[:, None]  # (F, 24, 3)\n",
    "width, height, K_fullimg = create_camera_sensor(1000, 1000, 24) \n",
    "wham_cam_augmentor = CameraAugmentorV11()\n",
    "T_w2c = wham_cam_augmentor(w_j3d, motion_frames_len) \n",
    "\n",
    "offset = smplx.get_skeleton(betas[0].to(device))[0]  # (3)\n",
    "global_orient_c, transl_c = get_c_rootparam(\n",
    "    global_orient_w, transl_w, \n",
    "    T_w2c, offset.cpu(),\n",
    ")\n",
    "smpl_params_c = {\n",
    "    \"body_pose\": smpl_params_w[\"body_pose\"].clone(),  # (F, 63)\n",
    "    \"betas\": smpl_params_w[\"betas\"].clone(),  # (F, 10)\n",
    "    \"global_orient\": global_orient_c,  # (F, 3)\n",
    "    \"transl\": transl_c,  # (F, 3)\n",
    "}\n",
    "        \n",
    "# World Params\n",
    "gravity_vec = torch.tensor([0, -1, 0], dtype=torch.float32)  # (3), BEDLAM is ay\n",
    "R_c2gv = get_R_c2gv(T_w2c[:, :3, :3], gravity_vec)  # (F, 3, 3)\n",
    "\n",
    "K_fullimg = K_fullimg.repeat(motion_frames_len, 1, 1)  # (F, 3, 3)\n",
    "cam_angvel = compute_cam_angvel(T_w2c[:, :3, :3])  # (F, 6)\n",
    "\n",
    "batch = {\n",
    "    \"meta\": {\"data_name\": \"amass\", \"idx\": idx, \"T_w2c\": T_w2c},\n",
    "    \"length\": data[\"body_pose\"].shape[0],\n",
    "    \"smpl_params_c\": smpl_params_c,\n",
    "    \"smpl_params_w\": smpl_params_w,\n",
    "    \"R_c2gv\": R_c2gv,  # (F, 3, 3)\n",
    "    \"gravity_vec\": gravity_vec,  # (3)\n",
    "    \"bbx_xys\": torch.zeros((data[\"body_pose\"].shape[0], 3)),  # (F, 3)  # NOTE: a placeholder\n",
    "    \"K_fullimg\": K_fullimg,  # (F, 3, 3)\n",
    "    \"f_imgseq\": torch.zeros((data[\"body_pose\"].shape[0], 1024)),  # (F, D)  # NOTE: a placeholder\n",
    "    \"kp2d\": torch.zeros(data[\"body_pose\"].shape[0], 17, 3),  # (F, 17, 3)\n",
    "    \"cam_angvel\": cam_angvel,  # (F, 6)\n",
    "    \"mask\": {\n",
    "        \"valid\": get_valid_mask(data[\"body_pose\"].shape[0], data[\"body_pose\"].shape[0]),\n",
    "        \"vitpose\": False,\n",
    "        \"bbx_xys\": False,\n",
    "        \"f_imgseq\": False,\n",
    "        \"spv_incam_only\": False,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b87df7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering Camera:   0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering Camera: 100%|██████████| 120/120 [00:01<00:00, 66.48it/s]\n"
     ]
    }
   ],
   "source": [
    "w_j3d = smplx(**{k:v.to(device) for k,v in smpl_params_w.items()}).joints.cpu()\n",
    "\n",
    "width, height, K_fullimg = create_camera_sensor(1000, 1000, 24) \n",
    "wham_cam_augmentor = CameraAugmentorV11()\n",
    "T_w2c = wham_cam_augmentor(w_j3d, motion_frames_len) \n",
    "\n",
    "c_j3d = apply_T_on_points(w_j3d[:,:22], T_w2c)\n",
    "verts, faces, vertex_colors = convert_motion_as_line_mesh(c_j3d)\n",
    "vertex_colors = vertex_colors[None] / 255.0\n",
    "bg = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "renderer = Renderer(width, height, device=\"cuda\", faces=faces, K=K_fullimg)\n",
    "writer = get_writer(f'tmp.mp4', fps=30, crf=23)\n",
    "for i in tqdm(range(motion_frames_len), desc=f\"Rendering Camera\"):\n",
    "    img_overlay_pred = renderer.render_mesh(verts[i].cuda(), bg, vertex_colors, VI=1)\n",
    "    writer.write_frame(img_overlay_pred)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adabe782",
   "metadata": {},
   "outputs": [],
   "source": [
    "smplx_out = smplx(**{\n",
    "    \"body_pose\": data[\"body_pose\"].to(device),  # (F, 63)\n",
    "    \"betas\": betas.to(device),  # (F, 10)\n",
    "    \"global_orient\": global_orient_w.to(device),  # (F, 3)\n",
    "    \"transl\": transl_w.to(device),  # (F, 3)\n",
    "})\n",
    "pred_ay_verts = torch.stack([torch.matmul(smplx2smpl, v_) for v_ in smplx_out.vertices])\n",
    "pred_gb_verts, pred_gb_joints = move_to_start_point_face_z(pred_ay_verts, J_regressor)\n",
    "\n",
    "global_R, global_T, global_lights = get_global_cameras_static(\n",
    "    pred_gb_joints.cpu(), beta=2.0, cam_height_degree=20, target_center_height=1.0,\n",
    ")\n",
    "_, _, K = create_camera_sensor(width, height, 24)\n",
    "renderer_g = Renderer(width, height, device=\"cuda\", faces=faces_smpl, K=K)\n",
    "\n",
    "# -- render mesh -- #\n",
    "scale, cx, cz = get_ground_params_from_points(pred_gb_joints[:, 0], pred_gb_verts)\n",
    "renderer_g.set_ground(scale * 1.5, cx, cz)\n",
    "color = torch.ones(3).float().cuda() * 0.8\n",
    "\n",
    "writer = get_writer(f'tmp.mp4', fps=30, crf=23)\n",
    "for i in tqdm(range(motion_frames_len), desc=f\"Rendering Global\"):\n",
    "    # img_overlay_pred = renderer.render_mesh(verts[i].cuda(), bg, vertex_colors, VI=1)\n",
    "    cameras = renderer_g.create_camera(global_R[i], global_T[i])\n",
    "    img_gb = renderer_g.render_with_ground(pred_gb_verts[[i]], color[None], cameras, global_lights)\n",
    "    writer.write_frame(img_gb)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b199ff76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total motion files: 37,537\n"
     ]
    }
   ],
   "source": [
    "### BEDLAM Train Dataset --Load Dataset-- ###\n",
    "\n",
    "root = Path(\"inputs/BEDLAM/hmr4d_support\")\n",
    "min_motion_frames = 60\n",
    "max_motion_frames = 120\n",
    "lazy_load=True\n",
    "random1024=False\n",
    "\n",
    "mid_to_valid_range = {}\n",
    "mid_to_imgfeat_dir = {}\n",
    "\n",
    "mid_to_valid_range_ = torch.load(root / \"mid_to_valid_range_all60.pt\")\n",
    "mid_to_valid_range.update(mid_to_valid_range_)\n",
    "mid_to_imgfeat_dir.update({mid: root / \"imgfeats/bedlam_all60\" for mid in mid_to_valid_range_})\n",
    "\n",
    "mid_to_valid_range_ = torch.load(root / \"mid_to_valid_range_maxspan60.pt\")\n",
    "mid_to_valid_range.update(mid_to_valid_range_)\n",
    "mid_to_imgfeat_dir.update({mid: root / \"imgfeats/bedlam_maxspan60\" for mid in mid_to_valid_range_})\n",
    "\n",
    "motion_files = torch.load(root / \"smplpose_v2.pth\")\n",
    "idx2meta = list(mid_to_valid_range.keys())\n",
    "\n",
    "print(f\"Total motion files: {len(idx2meta):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "560056b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pose': tensor[98, 66] n=6468 (25Kb) x∈[-0.946, 1.238] μ=-4.890e-05 σ=0.250,\n",
       " 'trans': tensor[98, 3] n=294 (1.1Kb) x∈[-0.121, 1.390] μ=0.441 σ=0.666,\n",
       " 'beta': tensor[10] x∈[-0.719, 0.932] μ=-0.109 σ=0.485 [0.932, -0.589, -0.404, -0.719, 0.136, 0.325, -0.053, -0.170, -0.408, -0.143],\n",
       " 'skeleton': tensor[22, 3] n=66 x∈[-1.365, 0.709] μ=-0.127 σ=0.390,\n",
       " 'trans_incam': tensor[98, 3] n=294 (1.1Kb) x∈[-0.663, 0.073] μ=-0.338 σ=0.287,\n",
       " 'global_orient_incam': tensor[98, 3] n=294 (1.1Kb) x∈[-0.948, 2.963] μ=0.642 σ=1.635,\n",
       " 'cam_ext': tensor[98, 4, 4] n=1568 (6.1Kb) x∈[-0.567, 2.956] μ=0.455 σ=0.796,\n",
       " 'cam_int': tensor[98, 3, 3] n=882 (3.4Kb) x∈[0., 995.556] μ=332.457 σ=412.101,\n",
       " 'start_end': (28, 126),\n",
       " 'length': 98,\n",
       " 'f_imgseq': tensor[98, 1024] n=100352 (0.4Mb) x∈[-5.503, 5.472] μ=0.019 σ=1.017,\n",
       " 'bbx_xys': tensor[98, 3] n=294 (1.1Kb) x∈[296.461, 816.233] μ=564.220 σ=191.347,\n",
       " 'img_wh': (720, 1280),\n",
       " 'kp2d': tensor[98, 17, 3] n=4998 (20Kb) \u001b[38;2;127;127;127mall_zeros\u001b[0m}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 30\n",
    "np.random.seed(42)\n",
    "\n",
    "mid = idx2meta[idx]\n",
    "data = motion_files[mid].copy()\n",
    "\n",
    "# Random select a subset\n",
    "range1, range2 = mid_to_valid_range[mid]  # [range1, range2)\n",
    "mlength = range2 - range1\n",
    "if mlength < min_motion_frames:  # the minimal mlength is 30 when generating data\n",
    "    start = range1\n",
    "    length = mlength\n",
    "else:\n",
    "    effect_max_motion_len = min(max_motion_frames, mlength)\n",
    "    length = np.random.randint(min_motion_frames, effect_max_motion_len + 1)  # [low, high)\n",
    "    start = np.random.randint(range1, range2 - length + 1)\n",
    "end = start + length\n",
    "data[\"start_end\"] = (start, end)\n",
    "data[\"length\"] = length\n",
    "for k, v in data.items():\n",
    "    if isinstance(v, torch.Tensor) and len(v.shape) > 1 and k != \"skeleton\":\n",
    "        data[k] = v[start:end]\n",
    "        \n",
    "# Load img(as feature) : {mid -> 'features', 'bbx_xys', 'img_wh', 'start_end'}\n",
    "imgfeat_dir = mid_to_imgfeat_dir[mid]\n",
    "f_img_dict = torch.load(imgfeat_dir / f\"{Path(mid).parts[-3]}/{Path(mid).parts[-1]}.pt\")\n",
    "start_mapped = start - f_img_dict[\"start_end\"][0]\n",
    "end_mapped = end - f_img_dict[\"start_end\"][0]\n",
    "data[\"f_imgseq\"] = f_img_dict[\"features\"][start_mapped:end_mapped].float()  # (L, 1024)\n",
    "data[\"bbx_xys\"] = f_img_dict[\"bbx_xys\"][start_mapped:end_mapped].float()  # (L, 4)\n",
    "data[\"img_wh\"] = f_img_dict[\"img_wh\"]  # (2)\n",
    "data[\"kp2d\"] = torch.zeros((end - start), 17, 3)  # (L, 17, 3)  # do not provide kp2d\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "607d692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_pose = data[\"pose\"][:, 3:]  # (F, 63)\n",
    "betas = data[\"beta\"].repeat(length, 1)  # (F, 10)\n",
    "\n",
    "global_orient = data[\"global_orient_incam\"]  # (F, 3)\n",
    "transl = data[\"trans_incam\"] + data[\"cam_ext\"][:, :3, 3]  # (F, 3), bedlam convention\n",
    "smpl_params_c = {\"body_pose\": body_pose, \"betas\": betas, \"transl\": transl, \"global_orient\": global_orient}\n",
    "\n",
    "# SMPL params in world\n",
    "global_orient_w = data[\"pose\"][:, :3]  # (F, 3)\n",
    "transl_w = data[\"trans\"]  # (F, 3)\n",
    "smpl_params_w = {\"body_pose\": body_pose, \"betas\": betas, \"transl\": transl_w, \"global_orient\": global_orient_w}\n",
    "\n",
    "# World Params\n",
    "offset = data[\"skeleton\"][0] # (3)\n",
    "T_w2c = get_T_w2c_from_wcparams(\n",
    "    global_orient_w=global_orient_w,\n",
    "    transl_w=transl_w,\n",
    "    global_orient_c=global_orient,\n",
    "    transl_c=transl,\n",
    "    offset=data[\"skeleton\"][0],\n",
    ")\n",
    "\n",
    "gravity_vec = torch.tensor([0, -1, 0], dtype=torch.float32)  # (3), BEDLAM is ay\n",
    "R_c2gv = get_R_c2gv(T_w2c[:, :3, :3], gravity_vec)  # (F, 3, 3)\n",
    "\n",
    "K_fullimg = data['cam_int'].repeat(motion_frames_len, 1, 1)  # (F, 3, 3)\n",
    "cam_angvel = compute_cam_angvel(T_w2c[:, :3, :3])  # (F, 6)\n",
    "\n",
    "batch = {\n",
    "    \"meta\": {\"data_name\": \"bedlam\", \"idx\": idx},\n",
    "    \"length\": length,\n",
    "    \"smpl_params_c\": smpl_params_c,\n",
    "    \"smpl_params_w\": smpl_params_w,\n",
    "    \"R_c2gv\": R_c2gv,  # (F, 3, 3)\n",
    "    \"gravity_vec\": gravity_vec,  # (3)\n",
    "    \"bbx_xys\": data[\"bbx_xys\"],  # (F, 3)\n",
    "    \"K_fullimg\": data[\"cam_int\"],  # (F, 3, 3)\n",
    "    \"f_imgseq\": data[\"f_imgseq\"],  # (F, D)\n",
    "    \"kp2d\": data[\"kp2d\"],  # (F, 17, 3)\n",
    "    \"cam_angvel\": cam_angvel,  # (F, 6)\n",
    "    \"mask\": {\n",
    "        \"valid\": get_valid_mask(max_motion_frames, length),\n",
    "        \"vitpose\": False,\n",
    "        \"bbx_xys\": True,\n",
    "        \"f_imgseq\": True,\n",
    "        \"spv_incam_only\": False,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca7dbde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering:   0%|          | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering: 100%|██████████| 98/98 [00:02<00:00, 46.36it/s]\n"
     ]
    }
   ],
   "source": [
    "smplx_out = smplx(**{k: v.to(device) for k,v in smpl_params_c.items()})\n",
    "\n",
    "# ----- Render Overlay ----- #\n",
    "render_dict = {\n",
    "    \"faces\": smplx.faces,\n",
    "    \"verts\": smplx_out.vertices,\n",
    "    'whf' : (1280, 720, 995.5555)\n",
    "}\n",
    "img_overlay = simple_render_mesh(render_dict)\n",
    "save_video(img_overlay, \"tmp.mp4\", crf=23)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
