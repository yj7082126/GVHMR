{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n",
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from einops import repeat\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import imageio.v3 as iio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from timm.models.vision_transformer import Mlp\n",
    "\n",
    "from hmr4d.dataset.h36m.h36m import H36mSmplDataset #6,196 samples\n",
    "from hmr4d.network.gvhmr.relative_transformer import NetworkEncoderRoPE\n",
    "from hmr4d.model.gvhmr.utils.endecoder import EnDecoder\n",
    "from hmr4d.model.gvhmr.utils import stats_compose\n",
    "from hmr4d.utils.body_model.smplx_lite import SmplxLiteV437Coco17\n",
    "from hmr4d.utils.geo.hmr_cam import compute_bbox_info_bedlam, compute_transl_full_cam\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a63dabdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m01/01 20:03:27\u001b[0m][\u001b[32mINFO\u001b[0m] [EnDecoder] Use MM_V1_AMASS_LOCAL_BEDLAM_CAM for statistics!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Main : model.gvhmr.gvhmr_pl.GvhmrPL\n",
    "    # Pipeline : model.gvhmr.pipeline.gvhmr_pipeline.Pipeline\n",
    "        # denoiser3d : network.gvhmr.relative_transformer.NetworkEncoderRoPE\n",
    "        # endecoder : model.gvhmr.utils.endecoder.EnDecoder\n",
    "    # Optimizer : adamw_2e-4\n",
    "\n",
    "denoiser3d = NetworkEncoderRoPE().eval().to(device)\n",
    "weights = torch.load(\"inputs/checkpoints/gvhmr/gvhmr_siga24_release.ckpt\")\n",
    "denoiser3d.load_state_dict({k.replace('pipeline.denoiser3d.', ''): v for k, v in weights['state_dict'].items() if 'denoiser3d.' in k})\n",
    "\n",
    "endecoder = EnDecoder(stats_name=\"MM_V1_AMASS_LOCAL_BEDLAM_CAM\").eval().to(device)\n",
    "\n",
    "cam_angvel_stats = stats_compose.cam_angvel[\"manual\"]\n",
    "cam_angvel_mean = torch.tensor(cam_angvel_stats[\"mean\"], device=device)\n",
    "cam_angvel_std = torch.tensor(cam_angvel_stats[\"std\"], device=device)\n",
    "\n",
    "params = []\n",
    "for k, v in denoiser3d.named_parameters():\n",
    "    if v.requires_grad:\n",
    "        params.append(v)\n",
    "optimizer = torch.optim.AdamW(params=params, lr=2e-4)\n",
    "scehedule_lr = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200, 350], gamma=0.5)\n",
    "\n",
    "smplx = SmplxLiteV437Coco17().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a52fe651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[36m01/01 19:55:46\u001b[0m][\u001b[32mINFO\u001b[0m] [H36M] Loading from inputs/H36M/hmr4d_support/smplxpose_v1.pt ...\u001b[0m\n",
      "[\u001b[36m01/01 19:55:46\u001b[0m][\u001b[32mINFO\u001b[0m] [H36M] 600 sequences. Elapsed: 0.59s\u001b[0m\n",
      "[\u001b[36m01/01 19:55:46\u001b[0m][\u001b[32mINFO\u001b[0m] [H36M] Fully Loading to RAM ViT-Feat: inputs/H36M/hmr4d_support/vitfeat_h36m.pt\u001b[0m\n",
      "[\u001b[36m01/01 19:55:47\u001b[0m][\u001b[32mINFO\u001b[0m] [H36M] Finished. Elapsed: 1.62s\u001b[0m\n",
      "[\u001b[36m01/01 19:55:47\u001b[0m][\u001b[32mINFO\u001b[0m] [H36M] has 8.7 hours motion -> Resampled to 6196 samples.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'data_name': 'h36m', 'idx': 3803, 'vid': 'S6@Sitting_1@58860488', 'start_end': [370, 490]}, {'data_name': 'h36m', 'idx': 3535, 'vid': 'S5@Directions_1@58860488', 'start_end': [2183, 2303]}, {'data_name': 'h36m', 'idx': 3780, 'vid': 'S6@Walking@58860488', 'start_end': [1591, 1711]}, {'data_name': 'h36m', 'idx': 5336, 'vid': 'S6@Walking@60457274', 'start_end': [1580, 1700]}, {'data_name': 'h36m', 'idx': 4341, 'vid': 'S7@Discussion@58860488', 'start_end': [1813, 1933]}, {'data_name': 'h36m', 'idx': 512, 'vid': 'S5@Posing_1@54138969', 'start_end': [703, 823]}, {'data_name': 'h36m', 'idx': 944, 'vid': 'S7@Photo@54138969', 'start_end': [833, 953]}, {'data_name': 'h36m', 'idx': 4452, 'vid': 'S8@Walking@58860488', 'start_end': [18, 138]}, {'data_name': 'h36m', 'idx': 5610, 'vid': 'S7@Purchases_1@60457274', 'start_end': [12, 132]}, {'data_name': 'h36m', 'idx': 5627, 'vid': 'S7@Sitting_1@60457274', 'start_end': [838, 958]}, {'data_name': 'h36m', 'idx': 856, 'vid': 'S6@Discussion@54138969', 'start_end': [87, 207]}, {'data_name': 'h36m', 'idx': 2127, 'vid': 'S5@Waiting_1@55011271', 'start_end': [1941, 2061]}, {'data_name': 'h36m', 'idx': 2419, 'vid': 'S6@Eating_2@55011271', 'start_end': [295, 415]}, {'data_name': 'h36m', 'idx': 399, 'vid': 'S5@Walking_1@54138969', 'start_end': [1240, 1360]}, {'data_name': 'h36m', 'idx': 4239, 'vid': 'S7@Posing_1@58860488', 'start_end': [133, 253]}, {'data_name': 'h36m', 'idx': 5575, 'vid': 'S7@Phoning@60457274', 'start_end': [589, 709]}, {'data_name': 'h36m', 'idx': 5935, 'vid': 'S7@Walking_2@60457274', 'start_end': [702, 822]}, {'data_name': 'h36m', 'idx': 3333, 'vid': 'S1@Sitting_2@58860488', 'start_end': [996, 1116]}, {'data_name': 'h36m', 'idx': 3092, 'vid': 'S8@Eating_1@55011271', 'start_end': [883, 1003]}, {'data_name': 'h36m', 'idx': 1653, 'vid': 'S1@WalkingDog_1@55011271', 'start_end': [270, 390]}, {'data_name': 'h36m', 'idx': 2519, 'vid': 'S7@WalkDog@55011271', 'start_end': [953, 1073]}, {'data_name': 'h36m', 'idx': 4798, 'vid': 'S1@Smoking@60457274', 'start_end': [428, 548]}, {'data_name': 'h36m', 'idx': 1744, 'vid': 'S1@Waiting_1@55011271', 'start_end': [490, 610]}, {'data_name': 'h36m', 'idx': 2360, 'vid': 'S6@WalkDog_1@55011271', 'start_end': [651, 771]}, {'data_name': 'h36m', 'idx': 1213, 'vid': 'S7@Directions@54138969', 'start_end': [874, 994]}, {'data_name': 'h36m', 'idx': 4827, 'vid': 'S1@Greeting_1@60457274', 'start_end': [410, 530]}, {'data_name': 'h36m', 'idx': 1107, 'vid': 'S7@SittingDown@54138969', 'start_end': [1158, 1278]}, {'data_name': 'h36m', 'idx': 811, 'vid': 'S6@WalkDog_1@54138969', 'start_end': [802, 922]}, {'data_name': 'h36m', 'idx': 1273, 'vid': 'S7@Eating_1@54138969', 'start_end': [550, 670]}, {'data_name': 'h36m', 'idx': 2306, 'vid': 'S6@SittingDown_1@55011271', 'start_end': [520, 640]}, {'data_name': 'h36m', 'idx': 5969, 'vid': 'S8@Phoning@60457274', 'start_end': [905, 1025]}, {'data_name': 'h36m', 'idx': 2818, 'vid': 'S7@Eating_1@55011271', 'start_end': [948, 1068]}]\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    # Assume all keys in the batch are the same\n",
    "    return_dict = {}\n",
    "    for k in batch[0].keys():\n",
    "        if k.startswith(\"meta\"):  # data information, do not batch\n",
    "            return_dict[k] = [d[k] for d in batch]\n",
    "        else:\n",
    "            return_dict[k] = default_collate([d[k] for d in batch])\n",
    "    return_dict[\"B\"] = len(batch)\n",
    "    return return_dict\n",
    "\n",
    "dataset = H36mSmplDataset()\n",
    "dataloader = DataLoader(\n",
    "    dataset, shuffle=True, num_workers=8,\n",
    "    persistent_workers=True, batch_size=32,\n",
    "    drop_last=True, collate_fn=collate_fn, pin_memory=True\n",
    ")\n",
    "np.random.seed(4)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "for k in batch.keys():\n",
    "    if k != 'meta':\n",
    "        if type(batch[k]) == dict:\n",
    "            for kk in batch[k].keys():\n",
    "                batch[k][kk] = batch[k][kk].to(device)\n",
    "        elif type(batch[k]) != int:\n",
    "            batch[k] = batch[k].to(device)\n",
    "print(batch['meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af9132d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.utils.geo.hmr_cam import perspective_projection, normalize_kp2d, safely_render_x3d_K, get_bbx_xys\n",
    "from hmr4d.utils.geo.augment_noisy_pose import (\n",
    "    get_wham_aug_kp3d,\n",
    "    get_visible_mask,\n",
    "    get_invisible_legs_mask,\n",
    "    randomly_modify_hands_legs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f960453",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, _ = batch[\"smpl_params_c\"][\"body_pose\"].shape[:2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    gt_verts437, gt_j3d = smplx(**batch[\"smpl_params_c\"])\n",
    "    root_ = gt_j3d[:, :, [11, 12], :].mean(-2, keepdim=True)\n",
    "    \n",
    "    batch[\"gt_j3d\"] = gt_j3d\n",
    "    batch[\"gt_cr_coco17\"] = gt_j3d - root_\n",
    "    batch[\"gt_c_verts437\"] = gt_verts437\n",
    "    batch[\"gt_cr_verts437\"] = gt_verts437 - root_\n",
    "    \n",
    "i_x2d = safely_render_x3d_K(gt_verts437, batch[\"K_fullimg\"], thr=0.3)\n",
    "bbx_xys = get_bbx_xys(i_x2d, do_augment=True)\n",
    "mask_bbx_xys = batch[\"mask\"][\"bbx_xys\"]\n",
    "batch[\"bbx_xys\"][~mask_bbx_xys] = bbx_xys[~mask_bbx_xys]\n",
    "\n",
    "noisy_j3d = gt_j3d + get_wham_aug_kp3d(gt_j3d.shape[:2])\n",
    "noisy_j3d = randomly_modify_hands_legs(noisy_j3d)\n",
    "obs_i_j2d = perspective_projection(noisy_j3d, batch[\"K_fullimg\"])  # (B, L, J, 2)\n",
    "\n",
    "j2d_visible_mask = get_visible_mask(gt_j3d.shape[:2]).cuda()  # (B, L, J)\n",
    "j2d_visible_mask[noisy_j3d[..., 2] < 0.3] = False  # Set close-to-image-plane points as invisible\n",
    "legs_invisible_mask = get_invisible_legs_mask(gt_j3d.shape[:2]).cuda()  # (B, L, J)\n",
    "j2d_visible_mask[legs_invisible_mask] = False\n",
    "\n",
    "obs_kp2d = torch.cat([obs_i_j2d, j2d_visible_mask[:, :, :, None].float()], dim=-1)  # (B, L, J, 3)\n",
    "obs = normalize_kp2d(obs_kp2d, batch[\"bbx_xys\"])  # (B, L, J, 3)\n",
    "obs[~j2d_visible_mask] = 0  # if not visible, set to (0,0,0)\n",
    "batch[\"obs\"] = obs\n",
    "batch[\"obs\"][~batch[\"mask\"][\"valid\"]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = batch[\"length\"]\n",
    "# *. Conditions\n",
    "cliff_cam = compute_bbox_info_bedlam(batch[\"bbx_xys\"], batch[\"K_fullimg\"])  # (B, L, 3)\n",
    "f_cam_angvel = (batch[\"cam_angvel\"] - cam_angvel_mean) / cam_angvel_std\n",
    "f_condition = {\n",
    "    \"obs\": batch[\"obs\"],  # (B, L, 17, 3)\n",
    "    \"f_cliffcam\": cliff_cam,  # (B, L, 3)\n",
    "    \"f_cam_angvel\": f_cam_angvel,  # (B, L, C=6)\n",
    "    \"f_imgseq\": batch[\"f_imgseq\"],  # (B, L, C=1024)\n",
    "}\n",
    "\n",
    "model_output = denoiser3d(length=length, **f_condition)  # pred_x, pred_cam, static_conf_logits\n",
    "decode_dict = endecoder.decode(model_output[\"pred_x\"])\n",
    "\n",
    "outputs = {\n",
    "    \"model_output\": model_output, \"decode_dict\": decode_dict, \n",
    "    \"pred_smpl_params_incam\" : {\n",
    "        \"body_pose\": decode_dict[\"body_pose\"],  # (B, L, 63)\n",
    "        \"betas\": decode_dict[\"betas\"],  # (B, L, 10)\n",
    "        \"global_orient\": decode_dict[\"global_orient\"],  # (B, L, 3)\n",
    "        \"transl\": compute_transl_full_cam(model_output[\"pred_cam\"], batch[\"bbx_xys\"], batch[\"K_fullimg\"]),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbc9cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmr4d.utils.net_utils import length_to_mask\n",
    "\n",
    "with torch.no_grad():\n",
    "    obs = f_condition[\"obs\"].clone()\n",
    "    B, L, J, C = obs.shape\n",
    "    \n",
    "    visible_mask = obs[..., [2]] > 0.5\n",
    "    obs[~visible_mask[..., 0]] = 0 \n",
    "\n",
    "    f_obs = denoiser3d.learned_pos_linear(obs[..., :2])  # (B, L, J, 32)\n",
    "    f_obs *= visible_mask \n",
    "    f_obs += denoiser3d.learned_pos_params.repeat(B, L, 1, 1) * ~visible_mask\n",
    "    x = denoiser3d.embed_noisyobs(f_obs.view(B, L, -1))  # (B, L, J*32) -> (B, L, C)\n",
    "\n",
    "    pmask = ~length_to_mask(length, L)\n",
    "    for block in denoiser3d.blocks:\n",
    "        x = block(x, attn_mask=None, tgt_key_padding_mask=pmask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d371a68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mlp(\n",
       "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (act): GELU(approximate='none')\n",
       "  (drop1): Dropout(p=0.0, inplace=False)\n",
       "  (norm): Identity()\n",
       "  (fc2): Linear(in_features=512, out_features=151, bias=True)\n",
       "  (drop2): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoiser3d.final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc8c62e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor grad MeanBackward0 cuda:0 0.100\n"
     ]
    }
   ],
   "source": [
    "# ========== Compute Loss ========== #\n",
    "total_loss = 0\n",
    "mask = batch[\"mask\"][\"valid\"]  # (B, L)\n",
    "\n",
    "# 1. Simple loss: MSE\n",
    "pred_x = model_output[\"pred_x\"]  # (B, L, C)\n",
    "target_x = endecoder.encode(batch)  # (B, L, C)\n",
    "simple_loss = F.mse_loss(pred_x, target_x, reduction=\"none\")\n",
    "mask_simple = mask[:, :, None].expand(-1, -1, pred_x.size(2)).clone()  # (B, L, C)\n",
    "mask_simple[batch[\"mask\"][\"spv_incam_only\"], :, 142:] = False  # 3dpw training\n",
    "simple_loss = (simple_loss * mask_simple).mean()\n",
    "total_loss += simple_loss\n",
    "print(simple_loss)\n",
    "\n",
    "# # 2. Extra loss\n",
    "# extra_funcs = [\n",
    "#     compute_extra_incam_loss,\n",
    "#     compute_extra_global_loss,\n",
    "# ]\n",
    "# for extra_func in extra_funcs:\n",
    "#     extra_loss, extra_loss_dict = extra_func(batch, outputs, self)\n",
    "#     total_loss += extra_loss\n",
    "#     outputs.update(extra_loss_dict)\n",
    "\n",
    "# outputs[\"loss\"] = total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Endecoder : gvhmr/v1_amass_local_bedlam_cam\n",
    "\n",
    "import torch.nn as nn\n",
    "from hmr4d.model.gvhmr.utils import stats_compose\n",
    "from hmr4d.utils.body_model.smplx_lite import SmplxLiteV437Coco17\n",
    "\n",
    "\n",
    "class EnDecoder(nn.Module):\n",
    "    def __init__(self, noise_pose_k=10):\n",
    "        super().__init__()\n",
    "        # Load mean, std\n",
    "        # tmp = stats_compose.MM_V1_AMASS_LOCAL_BEDLAM_CAM\n",
    "        elements = [\n",
    "            [stats_compose.body_pose_r6d, stats_compose.betas, stats_compose.global_orient_c_r6d, \n",
    "            stats_compose.global_orient_gv_r6d, stats_compose.local_transl_vel],\n",
    "            [\"amass\", \"amass\", \"bedlam\", \"bedlam\", \"amass\"]\n",
    "        ]\n",
    "        mean = [t[s][\"mean\"] for t, s in zip(elements[0], elements[1])]\n",
    "        mean = torch.tensor([x for xs in mean for x in xs]).float()\n",
    "        std = [t[s][\"std\"] for t, s in zip(elements[0], elements[1])]\n",
    "        std = torch.tensor([x for xs in std for x in xs]).float()\n",
    "        self.register_buffer(\"mean\", mean, False)\n",
    "        self.register_buffer(\"std\", std, False)\n",
    "\n",
    "        # option\n",
    "        self.noise_pose_k = noise_pose_k\n",
    "\n",
    "        # smpl\n",
    "        self.smplx_model = SmplxLiteV437Coco17()\n",
    "        parents = self.smplx_model.parents[:22]\n",
    "        self.register_buffer(\"parents_tensor\", parents, False)\n",
    "        self.parents = parents.tolist()\n",
    "\n",
    "    def get_noisyobs(self, data, return_type=\"r6d\"):\n",
    "        \"\"\"\n",
    "        Noisy observation contains local pose with noise\n",
    "        Args:\n",
    "            data (dict):\n",
    "                body_pose: (B, L, J*3) or (B, L, J, 3)\n",
    "        Returns:\n",
    "            noisy_bosy_pose: (B, L, J, 6) or (B, L, J, 3) or (B, L, 3, 3) depends on return_type\n",
    "        \"\"\"\n",
    "        body_pose = data[\"body_pose\"]  # (B, L, 63)\n",
    "        B, L, _ = body_pose.shape\n",
    "        body_pose = body_pose.reshape(B, L, -1, 3)\n",
    "\n",
    "        # (B, L, J, C)\n",
    "        return_mapping = {\"R\": 0, \"r6d\": 1, \"aa\": 2}\n",
    "        return_id = return_mapping[return_type]\n",
    "        noisy_bosy_pose = gaussian_augment(body_pose, self.noise_pose_k, to_R=True)[return_id]\n",
    "        return noisy_bosy_pose\n",
    "\n",
    "    def normalize_body_pose_r6d(self, body_pose_r6d):\n",
    "        \"\"\"body_pose_r6d: (B, L, {J*6}/{J, 6}) ->  (B, L, J*6)\"\"\"\n",
    "        B, L = body_pose_r6d.shape[:2]\n",
    "        body_pose_r6d = body_pose_r6d.reshape(B, L, -1)\n",
    "        if self.mean.shape[-1] == 1:  # no mean, std provided\n",
    "            return body_pose_r6d\n",
    "        body_pose_r6d = (body_pose_r6d - self.mean[:126]) / self.std[:126]  # (B, L, C)\n",
    "        return body_pose_r6d\n",
    "\n",
    "    def fk_v2(self, body_pose, betas, global_orient=None, transl=None, get_intermediate=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            body_pose: (B, L, 63)\n",
    "            betas: (B, L, 10)\n",
    "            global_orient: (B, L, 3)\n",
    "        Returns:\n",
    "            joints: (B, L, 22, 3)\n",
    "        \"\"\"\n",
    "        B, L = body_pose.shape[:2]\n",
    "        if global_orient is None:\n",
    "            global_orient = torch.zeros((B, L, 3), device=body_pose.device)\n",
    "        aa = torch.cat([global_orient, body_pose], dim=-1).reshape(B, L, -1, 3)\n",
    "        rotmat = axis_angle_to_matrix(aa)  # (B, L, 22, 3, 3)\n",
    "\n",
    "        skeleton = self.smplx_model.get_skeleton(betas)[..., :22, :]  # (B, L, 22, 3)\n",
    "        local_skeleton = skeleton - skeleton[:, :, self.parents_tensor]\n",
    "        local_skeleton = torch.cat([skeleton[:, :, :1], local_skeleton[:, :, 1:]], dim=2)\n",
    "\n",
    "        if transl is not None:\n",
    "            local_skeleton[..., 0, :] += transl  # B, L, 22, 3\n",
    "\n",
    "        mat = matrix.get_TRS(rotmat, local_skeleton)  # B, L, 22, 4, 4\n",
    "        fk_mat = matrix.forward_kinematics(mat, self.parents)  # B, L, 22, 4, 4\n",
    "        joints = matrix.get_position(fk_mat)  # B, L, 22, 3\n",
    "        if not get_intermediate:\n",
    "            return joints\n",
    "        else:\n",
    "            return joints, mat, fk_mat\n",
    "\n",
    "    def get_local_pos(self, betas):\n",
    "        skeleton = self.smplx_model.get_skeleton(betas)[..., :22, :]  # (B, L, 22, 3)\n",
    "        local_skeleton = skeleton - skeleton[:, :, self.parents_tensor]\n",
    "        local_skeleton = torch.cat([skeleton[:, :, :1], local_skeleton[:, :, 1:]], dim=2)\n",
    "        return local_skeleton\n",
    "\n",
    "    def encode(self, inputs):\n",
    "        \"\"\"\n",
    "        definition: {\n",
    "                body_pose_r6d,  # (B, L, (J-1)*6) -> 0:126\n",
    "                betas, # (B, L, 10) -> 126:136\n",
    "                global_orient_r6d,  # (B, L, 6) -> 136:142  incam\n",
    "                global_orient_gv_r6d: # (B, L, 6) -> 142:148  gv\n",
    "                local_transl_vel,  # (B, L, 3) -> 148:151, smpl-coord\n",
    "            }\n",
    "        \"\"\"\n",
    "        B, L = inputs[\"smpl_params_c\"][\"body_pose\"].shape[:2]\n",
    "        # cam\n",
    "        smpl_params_c = inputs[\"smpl_params_c\"]\n",
    "        body_pose = smpl_params_c[\"body_pose\"].reshape(B, L, 21, 3)\n",
    "        body_pose_r6d = matrix_to_rotation_6d(axis_angle_to_matrix(body_pose)).flatten(-2)\n",
    "        betas = smpl_params_c[\"betas\"]\n",
    "        global_orient_R = axis_angle_to_matrix(smpl_params_c[\"global_orient\"])\n",
    "        global_orient_r6d = matrix_to_rotation_6d(global_orient_R)\n",
    "\n",
    "        # global\n",
    "        R_c2gv = inputs[\"R_c2gv\"]  # (B, L, 3, 3)\n",
    "        global_orient_gv_r6d = matrix_to_rotation_6d(R_c2gv @ global_orient_R)\n",
    "\n",
    "        # local_transl_vel\n",
    "        smpl_params_w = inputs[\"smpl_params_w\"]\n",
    "        local_transl_vel = get_local_transl_vel(smpl_params_w[\"transl\"], smpl_params_w[\"global_orient\"])\n",
    "        # returns\n",
    "        x = torch.cat([body_pose_r6d, betas, global_orient_r6d, global_orient_gv_r6d, local_transl_vel], dim=-1)\n",
    "        x_norm = (x - self.mean) / self.std\n",
    "        return x_norm\n",
    "\n",
    "    def encode_translw(self, inputs):\n",
    "        \"\"\"\n",
    "        definition: {\n",
    "                body_pose_r6d,  # (B, L, (J-1)*6) -> 0:126\n",
    "                betas, # (B, L, 10) -> 126:136\n",
    "                global_orient_r6d,  # (B, L, 6) -> 136:142  incam\n",
    "                global_orient_gv_r6d: # (B, L, 6) -> 142:148  gv\n",
    "                local_transl_vel,  # (B, L, 3) -> 148:151, smpl-coord\n",
    "            }\n",
    "        \"\"\"\n",
    "        # local_transl_vel\n",
    "        smpl_params_w = inputs[\"smpl_params_w\"]\n",
    "        local_transl_vel = get_local_transl_vel(smpl_params_w[\"transl\"], smpl_params_w[\"global_orient\"])\n",
    "\n",
    "        # returns\n",
    "        x = local_transl_vel\n",
    "        x_norm = (x - self.mean[-3:]) / self.std[-3:]\n",
    "        return x_norm\n",
    "\n",
    "    def decode_translw(self, x_norm):\n",
    "        return x_norm * self.std[-3:] + self.mean[-3:]\n",
    "\n",
    "    def decode(self, x_norm):\n",
    "        \"\"\"x_norm: (B, L, C)\"\"\"\n",
    "        B, L, C = x_norm.shape\n",
    "        x = (x_norm * self.std) + self.mean\n",
    "\n",
    "        body_pose_r6d = x[:, :, :126]\n",
    "        betas = x[:, :, 126:136]\n",
    "        global_orient_r6d = x[:, :, 136:142]\n",
    "        global_orient_gv_r6d = x[:, :, 142:148]\n",
    "        local_transl_vel = x[:, :, 148:151]\n",
    "\n",
    "        body_pose = matrix_to_axis_angle(rotation_6d_to_matrix(body_pose_r6d.reshape(B, L, -1, 6)))\n",
    "        body_pose = body_pose.flatten(-2)\n",
    "        global_orient_c = matrix_to_axis_angle(rotation_6d_to_matrix(global_orient_r6d))\n",
    "        global_orient_gv = matrix_to_axis_angle(rotation_6d_to_matrix(global_orient_gv_r6d))\n",
    "\n",
    "        output = {\n",
    "            \"body_pose\": body_pose,\n",
    "            \"betas\": betas,\n",
    "            \"global_orient\": global_orient_c,\n",
    "            \"global_orient_gv\": global_orient_gv,\n",
    "            \"local_transl_vel\": local_transl_vel,\n",
    "        }\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f489af",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl = BodyModelSMPLH(\n",
    "    model_path=\"inputs/checkpoints/body_models\", model_type=\"smpl\",\n",
    "    gender=\"neutral\", num_betas=10, create_body_pose=False, \n",
    "    create_betas=False, create_global_orient=False, create_transl=False,\n",
    ").to(device)\n",
    "smplx = BodyModelSMPLX(\n",
    "    model_path=\"inputs/checkpoints/body_models\", model_type=\"smplx\",\n",
    "    gender=\"neutral\", num_pca_comps=12, flat_hand_mean=False,\n",
    ").to(device)\n",
    "smplx2smpl = torch.load(\"hmr4d/utils/body_model/smplx2smpl_sparse.pt\").to(device)\n",
    "faces_smpl = torch.from_numpy((smpl.faces).astype(\"int\")).unsqueeze(0).to(device)\n",
    "faces_smplx = torch.from_numpy((smplx.faces).astype(\"int\")).unsqueeze(0).to(device)\n",
    "J_regressor = torch.load(\"hmr4d/utils/body_model/smpl_neutral_J_regressor.pt\").to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
